<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="Io5PBEGCOEzWaL7oGOG-6-qHqYLZsvb7vAXvdoEF-1Y" />
    <meta charset="utf-8">
    <meta name="description"
          content="">
    <meta name="keywords" content="Grasp Detection, Multi-Scale Features, Contrastive Learning, Grasp Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MISCGrasp</title>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true}, "HTML-CSS": {minScaleAdjust: 100} });
    </script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/favicon.png"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <link rel="icon" href="./static/images/fig1_300.png" type="image/png">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://fanqyu.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
            </a>

            <!--      <div class="navbar-item has-dropdown is-hoverable">-->
            <!--        <a class="navbar-link">-->
            <!--          More Related Research-->
            <!--        </a>-->
            <!--        <div class="navbar-dropdown">-->
            <!--          <a class="navbar-item" href="https://pointw.github.io/equi_rl_page/">-->
            <!--            Equivairant Reinforcement Learning-->
            <!--          </a>-->
            <!--          <a class="navbar-item" href="https://pointw.github.io/equi_q_page/">-->
            <!--            Equivariant Q-Learning-->
            <!--          </a>-->
            <!--          <a class="navbar-item" href="https://pointw.github.io/equi_robot_page/">-->
            <!--            On Robot Equivariant Learning-->
            <!--          </a>-->
            <!--          <a class="navbar-item" href="https://pointw.github.io/extrinsic_page/">-->
            <!--            Extrinsic Equivariance-->
            <!--          </a>-->
            <!--        </div>-->
            <!--      </div>-->
        </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping </h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                          <a href="https://fanqyu.github.io/">Qingyu Fan</a><sup>1,2</sup>,
                        </span>
                        <span class="author-block">
                          <a href="https://sites.google.com/site/caiyinghao">Yinghao Cai</a><sup>1,2†</sup>,
                        </span>
                        <span class="author-block">
                          Chao Li<sup>3</sup>,
                        </span>
                        <span class="author-block">
                          Chunting Jiao<sup>3</sup>,
                        </span>
                        <span class="author-block">
                          Xudong Zheng<sup>3</sup>,
                        </span>   
                        <span class="author-block">
                          Tao Lu<sup>1</sup>,
                        </span>
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?hl=en&user=mpchz3sAAAAJ&view_op=list_works&sortby=pubdate ">Bin Liang</a><sup>3</sup>,
                        </span>
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=XL9j2UUAAAAJ&hl=en">Shuo Wang</a><sup>1,2</sup>
                        </span>  
                    </div>

                   <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</span><br>
                    <span class="author-block"><sup>2</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences</span><br>
                    <span class="author-block"><sup>3</sup>Qiyuan Lab</span>
                    <div>
                        <span class="author-block" style="color: gray;">† corresponding author</span>
                    </div>
                  </div>


                <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- PDF Link. -->
                      <span class="link-block">
                        <a href="PAPER_URL"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>
                      <span class="link-block">
                        <a href="ARXIV_URL"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="ai ai-arxiv"></i>
                          </span>
                          <span>arXiv</span>
                        </a>
                      </span>
                      <!-- Video Link. -->
                      <span class="link-block">
                        <a href="https://youtu.be/KXRjxByu6hM"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-youtube"></i>
                          </span>
                          <span>Video</span>
                        </a>
                      </span>
                      <!-- Code Link. -->
                      <span class="link-block">
                        <a href="https://github.com/Fanqyu/MiscGrasp"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                          </a>
                      </span>
                      <!-- Dataset Link. -->
                      <span class="link-block">
                        <a href="https://github.com/Fanqyu/MiscGrasp"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="far fa-images"></i>
                          </span>
                          <span>Data</span>
                          </a>
                    </div>
                  </div>
                
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-demo1">
                    <video poster="" id="demo1" autoplay controls muted loop playsinline >
                        <source src="./static/video/all_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo2">
                    <video poster="" id="demo2" autoplay controls muted loop playsinline >
                        <source src="./static/video/small_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo3">
                    <video poster="" id="demo3" autoplay controls muted loop playsinline >
                        <source src="./static/video/large_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo4">
                    <video poster="" id="demo4" autoplay controls muted loop playsinline >
                        <source src="./static/video/12_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo5">
                    <video poster="" id="demo5" autoplay controls muted loop playsinline >
                        <source src="./static/video/13_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo6">
                    <video poster="" id="demo6" autoplay controls muted loop playsinline >
                        <source src="./static/video/15_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo7">
                    <video poster="" id="demo7" autoplay controls muted loop playsinline >
                        <source src="./static/video/16_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo8">
                    <video poster="" id="demo8" autoplay controls muted loop playsinline >
                        <source src="./static/video/20_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>
                
                <div class="item item-demo9">
                    <video poster="" id="demo9" autoplay controls muted loop playsinline >
                        <source src="./static/video/3_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo10">
                    <video poster="" id="demo10" autoplay controls muted loop playsinline >
                        <source src="./static/video/4_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo11">
                    <video poster="" id="demo11" autoplay controls muted loop playsinline >
                        <source src="./static/video/6_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo12">
                    <video poster="" id="demo12" autoplay controls muted loop playsinline >
                        <source src="./static/video/9_cmprs.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p align="center">
                        <img src="./static/images/fig1_300.png" alt="" style="width:70%"></img>
                    </p>
                    <p>
                        Robotic grasping faces challenges in adapting to objects with varying shapes 
                        and sizes. In this paper, we introduce <strong>MISCGrasp</strong>, a volumetric grasping method 
                        that integrates multi-scale feature extraction with contrastive feature enhancement 
                        for self-adaptive grasping. We propose a query-based interaction between high-level 
                        and low-level features through the Insight Transformer, while the Empower Transformer 
                        selectively attends to the highest-level features, which synergistically strikes a 
                        balance between focusing on fine geometric details and overall geometric structures. 
                        Furthermore, MISCGrasp utilizes multi-scale contrastive learning to exploit similarities 
                        among positive grasp samples, ensuring consistency across multi-scale features. Extensive 
                        experiments in both simulated and real-world environments demonstrate that MISCGrasp 
                        outperforms baseline and variant methods in tabletop decluttering tasks.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Introduction</h2>
                <div class="content has-text-justified">
                    <ol type="i">
                        <li>
                            We introduce <strong>MISCGrasp</strong>, a framework that enables self-adaptive 6-DoF grasping by seamlessly 
                            integrating power and pinch grasps. By incorporating geometric features at multiple scales, 
                            MISCGrasp improves the ability of grasping to handle a wide variety of objects.
                        </li>
                        <li>
                            A self-supervised feature enhancement method is proposed which further exploits the potential 
                            of multi-scale geometric features and ensures consistency among them at  each level using contrastive learning.
                        </li>
                        <li>
                            We generate a grasping dataset rich in both power and pinch grasps using a geometrically diverse object set, which 
                            lays the foundation for  evaluating 6-DoF grasping of objects of different shapes and sizes. Through extensive 
                            experiments, we demonstrate that MISCGrasp significantly outperforms baseline methods and variants, especially in 
                            scenarios suitable for pinch grasp.
                        </li>
                    </ol>
                    <p align="center">
                        <img src="./static/images/fig2_300.png"></img>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Simulation Experiments</h2>
                <div class="content has-text-justified">
                    <p>
                        We conduct four grasping experiments using four object sets: the EGAD test set, the pile and packed 
                        test set, and the adversarial object set. These experiments focus on both pinch and power grasps across 
                        two scene type: <strong>Pile</strong> and <strong>Packed</strong>. The <strong>EGAD-Single</strong> experiment scales the EGAD test set 
                        to 147 objects, ranked by geometric complexity and grasp difficulty, and evaluates performance on easy, 
                        medium, and hard sets. The <strong>Pile-Pile</strong> and <strong>Packed-Packed</strong> experiments use the pile and packed test 
                        sets with 10 objects per scene, increasing complexity. The <strong>EGAD+Adv-Pile</strong> experiment combines the 
                        EGAD test set and adversarial object set to test pinch grasp in pile scenes with 10 objects per scene.
                    </p>
                </div>
                <div class="column is-centered">
                    <img src="./static/images/sim_type.png" style="width:100%"></img>
                </div>

                <div class="content has-text-centered">
                    <p>
                        $\textbf{Visualization of EGAD+ADV-Pile}$
                    </p>
                </div>

                <div class="columns is-centered">
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/egad+adv-pile_27.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/egad+adv-pile_78.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/egad+adv-pile_79.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                </div>

                <div class="content has-text-centered">
                    <p>
                        $\textbf{Visualization of Pile-Pile}$
                    </p>
                </div>

                <div class="columns is-centered">
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/pile-pile_39.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/pile-pile_74.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/pile-pile_76.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>

                </div>
                
                <div class="content has-text-centered">
                    <p>
                        $\textbf{Visualization of Packed-Packed}$
                    </p>
                </div>

                <div class="columns is-centered">

                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/packed-packed_28.mp4" style="border-radius:10px;"></video>
                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/packed-packed_61.mp4" style="border-radius:10px;"></video>
                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/packed-packed_76.mp4" style="border-radius:10px;"></video>
                        </div>
                    </div>

                </div>

<!--                 <div class="column is-centered">
                    <img src="./static/images/sim_result.png"></img>
                </div>
                <div class="content has-text-justified">
                    <p>
                        We compared the OrbitGrasp with various baselines in terms of grasp success rate (GSR) and
                        declutter rate (DR). OrbitGrasp (3M) is trained on the downsampled 3M dataset. OrbitGrasp (6M)
                        is trained on
                        the full 6M dataset.
                        The results indicate that our method outperforms all
                        baselines across both settings and tasks in terms of both GSR and DR, in both the 3M and 6M
                        training sets. The high GSR indicates that our model can predict accurate grasp quality. On the
                        other hand, the high DR signifies that our model infers accurate grasp poses that do not move
                        objects outside of the workspace.
                    </p>
                </div>
            </div> -->
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Physical Experiments</h2>
                <div class="content has-text-justified">
                    <p>
                        Our experimental platform uses a UR5 robotic arm with a Robotiq 2-Finger 85 gripper, and grasp planning is 
                        performed in a 0.4 × 0.4 × 0.4 m<sup>3</sup> workspace, with perception from an Intel RealSense 
                        D435 depth sensor. We conduct both single-object and multi-object grasping experiments. Single-object tests 
                        involve five EGAD objects 3D-printed at three sizes (3 cm, 6 cm, 9 cm) for power and pinch grasps. Multi-object 
                        experiments include 9 EGAD objects, 14 Berkeley adversarial objects, 30 household items, and the single-object 
                        set, all tested in pile scenarios to optimize resources and time. Each test is repeated 20 times with 10 objects per round.
                    </p>
                </div>
                <div class="column is-centered" style="margin-top: -20px;">
                    <img src="./static/images/real_obj.png"></img>
                </div>
<!--                 <div class="content has-text-centered" style="margin-top: 30px;">
                    <p>
                        $\textbf{Result Visualization}$
                    </p>
                </div>

                <div class="columns is-centered">
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" controls="" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/packed1.mp4" style="border-radius:10px;"></video>
                            <p style="text-align: center; font-size: 18px">Packed Scene 1</p>
                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" controls="" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/packed2.mp4" style="border-radius:10px;"></video>
                            <p style="text-align: center; font-size: 18px">Packed Scene 2</p>
                        </div>
                    </div>
                </div>

                <div class="columns is-centered">
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" controls="" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/pile1.mp4" style="border-radius:10px;"></video>
                            <p style="text-align: center; font-size: 18px">Pile Scene 1</p>
                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" controls="" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/pile2.mp4" style="border-radius:10px;"></video>
                            <p style="text-align: center; font-size: 18px">Pile Scene 2</p>
                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" controls="" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/video/pile3.mp4" style="border-radius:10px;"></video>
                            <p style="text-align: center; font-size: 18px">Pile Scene 3</p>
                        </div>
                    </div>


                </div>
                <div class="content has-text-centered" style="margin-top: 40px;">
                    <p>We compared the results of OrbitGrasp with VNEdgeGrasp using the same metrics as in the
                        simulation experiments.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <p>
                        $\textbf{Quantitative Results}$
                    </p>
                </div>
                <div class="column is-centered" style="margin-top: -25px;">
                    <img src="./static/images/real_result.png"></img>
                </div> -->
                
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Video</h2>
                <center>
                    <iframe width="840" height="472.5"
                            src="https://www.youtube.com/embed/QgVFuz5xMZE?si=s4diVjer74cY8f_6"
                            title="YouTube video player" frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                </center>
            </div>
        </div>
    </div>
</section>

<!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h3 class="title">Citation</h3>
        <pre><code>
@inproceedings{
   huorbitgrasp,
   title={OrbitGrasp: SE (3)-Equivariant Grasp Learning},
   author={Hu, Boce and Zhu, Xupeng and Wang, Dian and Dong, Zihao and Huang, Haojie and Wang, Chenghao and Walters, Robin and Platt, Robert},
   booktitle={8th Annual Conference on Robot Learning},
   year={2024},
   url={https://openreview.net/forum?id=clqzoCrulY}
}
    </code></pre>
    </div>
</section> -->
    
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
              Thanks to the authors of <a href="https://github.com/ethz-asl/vgn">VGN</a> for making their work publicly available. <br>
              Also, thanks to Boce, the creator of <a href="https://orbitgrasp.github.io/">OribitGrasp</a>, for sharing screen recording tips.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<footer class="footer">
    <div class="container">

        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                       This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
